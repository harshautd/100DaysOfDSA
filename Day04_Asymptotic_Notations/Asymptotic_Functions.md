# Decreasing Functions (Asymptotic Notation)

## **Common Decreasing Functions (Ordered by Rate of Decay)**

### **1. Constant Decay (No Change)**
- **Notation:** `O(1)`
- **Example:** Fixed operations that do not depend on `n`.

### **2. Inverse Logarithmic Decay**
- **Notation:** `O(1 / log n)`
- **Example:** Some probability distributions (e.g., Zipfâ€™s law).

### **3. Inverse Polynomial Decay**
- **Notation:** `O(1 / n^c)`, where `c > 0`
- **Examples:**
  - `O(1 / n)` â†’ Harmonic series.
  - `O(1 / n^2)` â†’ Inverse square laws (e.g., gravitational force).

### **4. Exponential Decay**
- **Notation:** `O(1 / 2^n)`
- **Example:** Radioactive decay, recursive algorithms with exponential shrinking.

### **5. Factorial Decay**
- **Notation:** `O(1 / n!)`
- **Example:** Taylor series remainder terms.

### **6. Double Exponential Decay**
- **Notation:** `O(1 / 2^{2^n})`
- **Example:** Extreme cases in combinatorics and cryptography.

## **Ordered List from Slowest to Fastest Decay**
```plaintext
O(1) â‰« O(1 / log n) â‰« O(1 / n) â‰« O(1 / n^2) â‰« O(1 / 2^n) â‰« O(1 / n!) â‰« O(1 / 2^{2^n})
```

## **Real-Life Applications**
| Notation | Example |
|----------|---------|
| `O(1 / log n)` | Zipfâ€™s law, probability distributions |
| `O(1 / n)` | Harmonic series, some economic models |
| `O(1 / n^2)` | Gravitational force (inverse square law) |
| `O(1 / 2^n)` | Radioactive decay, dynamic programming optimizations |
| `O(1 / n!)` | Taylor series remainder terms |
| `O(1 / 2^{2^n})` | Cryptography, theoretical combinatorics |

# Comparing Growth Rates: log(n) vs. n^(1/2)

## **1. Limit Approach**
To compare `log(n)` and `n^(1/2)`, we analyze their growth rates as `n` increases.
We take the limit of their ratio:

![equation](https://latex.codecogs.com/png.latex?\lim_{n%20\to%20\infty}%20\frac{\log%20n}{n^{1/2}})

Using L'HÃ´pital's Rule, differentiate the numerator and denominator:

![equation](https://latex.codecogs.com/png.latex?\frac{d}{dn}%20(\log%20n)%20=%20\frac{1}{n})

![equation](https://latex.codecogs.com/png.latex?\frac{d}{dn}%20(n^{1/2})%20=%20\frac{1}{2%20\sqrt{n}})

Applying L'HÃ´pital's Rule:

![equation](https://latex.codecogs.com/png.latex?\lim_{n%20\to%20\infty}%20\frac{\log%20n}{n^{1/2}}%20=%20\lim_{n%20\to%20\infty}%20\frac{1/n}{1/2\sqrt{n}}%20=%20\lim_{n%20\to%20\infty}%20\frac{2\sqrt{n}}{n}%20=%20\lim_{n%20\to%20\infty}%20\frac{2}{\sqrt{n}})

Since 2/âˆšn â†’ 0 as n â†’ âˆž, we conclude:

![equation](https://latex.codecogs.com/png.latex?\log%20n%20\ll%20n^{1/2})

which means `log(n)` grows much more slowly than `n^(1/2)`.

## **2. Asymptotic Notation**

![equation](https://latex.codecogs.com/png.latex?\log%20n%20=%20O(n^{1/2}))

More precisely,

![equation](https://latex.codecogs.com/png.latex?\log%20n%20=%20o(n^{1/2}))

meaning `log(n)` is asymptotically smaller than `n^(1/2)`.

## **3. Numerical Check**
For small values of `n`:

| `n`     | `log(n)` (base `e`) | `n^(1/2)` |
|---------|---------------------|-----------|
| 10      | 2.3                 | 3.16      |
| 100     | 4.6                 | 10        |
| 1000    | 6.9                 | 31.6      |
| 10,000  | 9.2                 | 100       |

Clearly, `n^(1/2)` grows much faster than `log(n)`.

## **Conclusion**
- `n^(1/2)` grows much faster than `log(n)`.
- For large `n`, `log(n)` is negligible compared to `n^(1/2)`.
- In algorithmic complexity, an `O(sqrt(n))` algorithm is significantly slower than an `O(log n)` algorithm.

ðŸš€ **Big-O Notation Matters!**

# Comparing logâ‚‚n and logâ‚ƒn Growth Rates

To compare logâ‚‚n and logâ‚ƒn, let's analyze their growth rates.

## 1. Change of Base Formula

Using the logarithm base change formula:
```
log_a(n) = log_b(n) / log_b(a)
```

We can express logâ‚ƒn in terms of logâ‚‚n:
```
logâ‚ƒn = logâ‚‚n / logâ‚‚(3)
```

Since logâ‚‚(3) â‰ˆ 1.585 > 1, we conclude:
```
logâ‚ƒn < logâ‚‚n
```

for all n > 1.

## 2. Limit Approach

We take the limit of their ratio:
```
lim(nâ†’âˆž) [logâ‚‚n / logâ‚ƒn]
```

Substituting logâ‚ƒn = logâ‚‚n / logâ‚‚(3):
```
lim(nâ†’âˆž) [logâ‚‚n / (logâ‚‚n / logâ‚‚(3))] = logâ‚‚(3) â‰ˆ 1.585
```

Since this is a constant, we conclude:
```
logâ‚‚n = Î˜(logâ‚ƒn)
```

which means they grow at the **same rate asymptotically**.

## 3. Asymptotic Notation

Since the ratio converges to a constant, we write:
```
logâ‚‚n = Î˜(logâ‚ƒn)
```

which implies that both functions grow at the same rate, up to a constant factor.

## 4. Numerical Comparison

```
| n  | logâ‚‚n | logâ‚ƒn |
|----|-------|-------|
| 2  | 1     | 0.631 |
| 4  | 2     | 1.262 |
| 8  | 3     | 1.893 |
| 16 | 4     | 2.524 |
| 32 | 5     | 3.155 |
```

Clearly, logâ‚‚n is always larger, but they both grow **slowly and proportionally**.

## If the leading terms of two functions differ only by a constant factor, then Big-O (O), Big-Omega (Î©), and Theta (Î˜) notations can be applied.
